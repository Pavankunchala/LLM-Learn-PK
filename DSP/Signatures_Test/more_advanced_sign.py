import dspy 




from dspy import (
    InputField,
    OutputField,
    Predict,
    ChainOfThought,
    Signature,
    settings,
    ChainOfThoughtWithHint,
    Prediction,
    ProgramOfThought,
    ReAct,
    MultiChainComparison
)
from rich import print


# Please UNCOMMENT sections of the code below to make it run 

#defining the local llms 
lm = dspy.OllamaLocal(model='llama3',max_tokens=4000)
colbertv2_wiki17_abstracts = dspy.ColBERTv2(url='http://20.102.90.50:2017/wiki17_abstracts')
dspy.settings.configure(lm=lm, rm=colbertv2_wiki17_abstracts)


## PREDITCT
#now lets try to experiment with different words and see how predict gives the results

predict_qa = Predict("question -> answer_as_json")

question = "Give me the top 5 countries based on their litreacy rate and happiness "




# #first lets see how predict takes the input 
# print(predict_qa)


# #now also the output it will give 
# test_predict = predict_qa(question = question)

# print("Json output: ",test_predict.answer_as_json)

##CHAINTOFTHOUGHT
#now lets do it thje same for Chain Of Thought and also  get more outputs with n 

# chain_qa = ChainOfThought("question -> answer_as_json",temperature =0.7, n= 2)

# print(chain_qa)

# test_chain = chain_qa(question = question)


# # to get the first response and more continious output 
# print(test_chain.completions.answer_as_json[0])
# print(test_chain.completions.answer_as_json[1])


#CHAINTOFTHOUGHT WITH HINT
# Lets deal with Chain Of Thought with  a Hint 
#terribe hint  I know that
# Hint = "Refer to the Internet and World Data "

# chain_hint_qa = ChainOfThoughtWithHint("question -> answer_as_json")


# hint_test = chain_hint_qa(question = question , hint = Hint)


# print("Chain of thought with Hint", hint_test.answer_as_json)




## MULTI COMPARISION
### Lets look into the code for multi_comparision  Honestly copied an example from some the documentation 
class BasicQA(Signature):
    """Give me detailed answers with the reasioning for it as well """
    question = InputField()
    answer = OutputField(desc = "Give detailed answers with the reasioning for it as well")

# # Example completions generated by a model for reference
# completions = [
#     Prediction(rationale="I recall that during clear days, the sky often appears this color.",
#                answer="blue"),
#     Prediction(rationale="Based on common knowledge, I believe the sky is typically seen as this color.",
#                answer="green"),
#     Prediction(rationale="From images and depictions in media, the sky is frequently represented with this hue.",
#                answer="blue"),
# ]

# # Pass signature to MultiChainComparison module
# compare_answers = MultiChainComparison(BasicQA)

# # Call the MultiChainComparison on the completions
# question = 'What is the color of the sky?'
# final_pred = compare_answers(completions, question=question)

# print(f"Question: {question}")
# print(f"Final Predicted Answer (after comparison): {final_pred.answer}")
# print(f"Final Rationale: {final_pred.rationale}")



#now lets look into program of thought 


#just trying a a different prompt 
# new_signature = Signature("question -> answer")

# pot = ProgramOfThought(new_signature)


# question = "Sara had 5 apples and gave 2 to her friend. Later in the day, she bought a dozen more apples but realized that she needed to give half of her total apples to her brother. After giving the apples to her brother, she found 3 more apples under a tree. How many apples does Sara have at the end of the day?"

# answer_pot = pot(question = question)

# print(f"Question: {question}")
# print("Program of thought answer:", answer_pot.answer)


# # qa = Predict("question")
# # Pass signature to ReAct module
# react_module = dspy.ReAct(BasicQA)

# # Call the ReAct module on a particular input
query = 'what is the most important phenomena that holds life together'
# result = react_module(question=question)

# print(f"Question: {question}")
# #idk why the answer is not coming for this one so we can inspect the histor for it but I will try to figujre it out and update it 
# print(f"Final Predicted Answer (after ReAct process): {result.answer}")

# lm.inspect_history(n=6)


# # Call the retriever on a particular query.
retrieve = dspy.Retrieve(k=3)
topK_passages = retrieve(query).passages
raw_retrival = retrieve(query)

# print(f"Top {retrieve.k} passages for question: {query} \n", '-' * 30, '\n')

# for idx, passage in enumerate(topK_passages):
#     print(f'{idx+1}]', passage, '\n')

retrival_qa = Predict("question, context -> answer")


output_retr = retrival_qa(question = query, context = raw_retrival.passages)

print(output_retr)